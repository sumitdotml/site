---
title: "Chasing Copy Heads in a Residual Streams Avenue"
description: "mechanics behind copy heads across different layers and their importance"
breadcrumbTitle: "Copy Heads in Transformers"
image: ""
pubDate: 2025-11-05
dateLabels:
  published: "Last updated: "
---

import HR from "../../components/HR.astro";
import { Image } from "astro:assets";
import doNothing from "../../assets/blog/copy-heads/do-nothing.png";

> WIP

I've been looking into attention mechanics recently, and this is my list of questions I want to answer in relation to copy heads to document my understanding:

### 1. Definitions

copy heads, the OV circuit, frobenius distance, identity matrix, singular value decomposition, etc.

### 2. Background of all this

- how did I reach to this point of thinking about copy heads?
- the essence of `W_OV`: why we need it, what it does, what it signifies (identity matrix relevance, wwhether there is transformation or whether the information from the incoming residual stream is merely getting put into the blender, when `W_OV` is essential: is it calculated at any point during training or inference?)

### 3. Importance of copy heads across different layers

- how do I know when a head is a copy head? more formalized and comprehensive than the definition
- is a copy head in, say, layer 1, as important as that in, say, layer 12 of a transformer?
- is a copy head even essential at all? why don't we simply focus on transform heads?

### 4. Critical questions

- Do we not pass all the new features that are learned in specific heads (in turn specific layers) to the residual stream?
- Does this not mean the features are always there in the residual stream for, say, head 3 in layer 13 if it wants to look up the a specific feature information from, say, head 6 in layer 1? why specifically require a copy head here? (mostly to do with row specifications -> residual stream is like a spreadsheet of rows and columns -> what if we want the info on row 3 of head 1.6 when we are in row 42 of head 13.3? can't simply utilize the residual stream -> attention is how we copy info from a row to the other, no?)

> All answers to be expanded with concrete examples (python, torch, visual diagrams).

Got nerd sniped by copy heads; I couldn't stop thinking of this meme in the process & thought this applied to them lol

<Image
  src={doNothing}
  alt="do-nothing-win"
  width={400}
  height={400}
  priority
  loading="eager"
/>

<HR weight="bold" colorStrength="solid" />

I found copy heads pretty interesting as I was going through some mechanical interpretability basics; I had jotted down some questions on a piece of paper for testing myself at the end of my learning to get a better intuition behind this concept, and I write this log answering all those questions as a reinforcement of my understanding.

## Definitions

Before diving into the mechanics behind copy heads across different transformer layers and their importance, some definitions are necessary to establish the foundations. Some key definitions coming up.

### Copy heads

Copy heads are attention heads. An attention head, in a transformer, exists inside a layer (or a block), and is a factor of the transformer's embedding dimension (`d_model` in the original transformer paper) that stores rich representations & semantic features during the training.



> I need to add an illustration of a transformer block here\
> and also an illustration of attention heads



Let's assume that I have an input text called: _"I like hot"_.

This is an input word, at the very start of the process, before any embeddings, before any forward passes.

Parameters that I'd like to assume for the sake of this example:

`batch` = 1 (since we have but a single sentence)\
`seqlen` = 3 (sequence length the size of input text)\
`d_model` = 32 (this will be the embedding dimension for every word, i.e., input token id)\
`n_layers` = 8 (total number of transformer blocks)\
`n_heads` = 4 (number of heads in each layer, factor of `d_model`, will expand more soon)

In a transformer,
