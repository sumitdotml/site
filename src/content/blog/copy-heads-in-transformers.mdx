---
title: "Chasing Copy Heads in a Residual Streams Avenue"
description: "mechanics behind copy heads across different layers and their importance"
breadcrumbTitle: "Copy Heads in Transformers"
image: ""
pubDate: 2025-11-05
dateLabels:
  published: "Last updated: "
---

import HR from "../../components/HR.astro";
import { Image } from "astro:assets";
import doNothing from "../../assets/blog/copy-heads/do-nothing.png";

> WIP

I've been looking into attention mechanics recently, and this is my list of questions I want to answer in relation to copy heads to document my understanding:

### 1. Definitions

copy heads, the OV circuit, frobenius distance, identity matrix, singular value decomposition, etc.

### 2. Background of all this

- how did I reach to this point of thinking about copy heads?
- the essence of `W_OV`: why we need it, what it does, what it signifies (identity matrix relevance, wwhether there is transformation or whether the information from the incoming residual stream is merely getting put into the blender, when `W_OV` is essential: is it calculated at any point during training or inference?)

### 3. Importance of copy heads across different layers

- how do I know when a head is a copy head? more formalized and comprehensive than the definition
- is a copy head in, say, layer 1, as important as that in, say, layer 12 of a transformer?
- is a copy head even essential at all? why don't we simply focus on transform heads?

### 4. Critical questions

- Do we not pass all the new features that are learned in specific heads (in turn specific layers) to the residual stream?
- Does this not mean the features are always there in the residual stream for, say, head 3 in layer 13 if it wants to look up the a specific feature information from, say, head 6 in layer 1? why specifically require a copy head here? (mostly to do with row specifications -> residual stream is like a spreadsheet of rows and columns -> what if we want the info on row 3 of head 1.6 when we are in row 42 of head 13.3? can't simply utilize the residual stream -> attention is how we copy info from a row to the other, no?)

> All answers to be expanded with concrete examples (python, torch, visual diagrams).

Got nerd sniped by copy heads; I couldn't stop thinking of this meme in the process & thought this applied to them lol

<Image
  src={doNothing}
  alt="do-nothing-win"
  width={400}
  height={400}
  priority
  loading="eager"
/>

<HR weight="bold" colorStrength="solid" />

I found copy heads pretty interesting as I was going through some mechanical interpretability basics; I had jotted down some questions on a piece of paper for testing myself at the end of my learning to get a better intuition behind this concept, and I write this log answering all those questions as a reinforcement of my understanding.

## Definitions

Before diving into the mechanics behind copy heads across different transformer layers and their importance, some definitions are necessary to establish the foundations. Some key definitions coming up.

### Copy heads

Copy heads are attention heads that are identity matrices or are close to being identity matrices. Inside a multi-head attention phase, we split the embedded dimensions into specific number of heads and do a parallel computation: in each head, we calculate the attention weights, do a matmul between attention weights and the `V` (here `V = x @ W_V`) to calculate attention output and then when we do another matmul between the attention output matrix and a projection matrix `W_O`, we get what we call an `output projection`.

Sometimes, an attention head barely transforms any information that it receives from the [residual stream](../inside-a-transformer); what it outputs could simply be a copy (or close to a copy) of what it receives from the residual stream. This sort of an attention head is called a copy head.

In order to get a clear picture behind a copy head, I think it's a good idea to get a quick refresher behind how we reach the point of an attention head.

### Refresher: How did we reach this rabbit hole?

An attention head, in a transformer, exists inside a layer (or a block), and is a factor of the transformer's embedding dimension (`d_model` in the original transformer paper) that stores rich representations & semantic features during the training.

> I need to add an illustration of a transformer block here\
> and also an illustration of attention heads

Let's assume that I have an `input_text` called: _"I like hot"_.

This is an input word, at the very start of the process, before any embeddings, before any forward passes.

Parameters that I'd like to assume for the sake of this example:

`batch` = 1 (since we have but a single sentence)\
`seqlen` = 3 (sequence length the size of input text)\
`d_model` = 32 (this will be the embedding dimension for every word, i.e., input token id)\
`n_layers` = 8 (total number of transformer blocks)\
`n_heads` = 4 (number of heads in each layer, factor of `d_model`, will expand more soon)

In a transformer, the `input_text` gets tokenized, which is simply a method to assign unique numeric representation to words or characters. Tokenization can be done via various methods, and not all of them break down a sequence of words (i.e., the `input_text here`) 1:1 into tokens but for the sake of this worklog, let me assume that each word from the `input_text` above corresponds to 1 token, kind of like:

```python
tokenizer = {
    "I": 1,
    "like": 2,
    "hot": 3,
} # a simple lookup table
```

This is still, to some extent, an oversimplification, but let's think of tokenization as a step where we create a lookup table (like a dictionary) of words (i.e., tokens) and their unique `IDs`.

At this point, if we think of this input sequence of unique token IDs as a tensor, it would be:

```python
tokens = torch.tensor([1, 2, 3])
tokens.shape # torch.Size([3])
```

Transformers do computation parallelly in batches, so generally, inputs are tokenized in batches as well. Basically, a single 'batch' represents a collection of tokens, and in a normal LLM, during training, there are countless number of batches, and each batch holds an enormous size of text (tons of sentences). In our case, since we have but just `input_text`, we are assuming that we have 1 batch, and that means our sequence length will be the length of our tokens, i.e., 3.

So I unsqueeze the `tokens` to better represent it as `[batch, seqlen]`:

```python
tokens = tokens.unsequeeze(0)
tokens # torch.tensor([[1, 2, 3]])
tokens.shape # torch.Size([1, 3])
```

So this new `tokens` represents the tokenized text that has 1 batch, and the batch's sequence length is 3.

Moving on, we (tbc)
